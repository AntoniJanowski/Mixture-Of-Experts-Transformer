{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mixture of Experts is an architecture designed to replace standard dense Feed Forward layers. It is most widely used in natural language processing inside transformers. The idea is that instead of having a normal, dense FF layer, we have few \"expert\" FF layers instead. When a token enters a MOE layer, it is directed to one (or more) of the experts by a router module. Because of that, during a forward pass, the amount of compute needed is the same as if we were using a normal FF layer (plus the router calculation, but it is a very small cost). But our models can be sizably bigger. \n",
        "\n",
        "This allows us to train bigger models under the same compute budget, making better use of available memory. MOE models are faster to pre-train than dense models and are widely used in many modern transformers (eg Deep Seek uses a MOE architecture). There is also evidence that particular experts sometimes specialize in handling certain kinds of tokens, improving the overall efficiency of the model. \n",
        "\n",
        "Bellow is the implementation of Mixture of Experts layer. It contains:\n",
        "* A naive, loop based implementation of MoE\n",
        "* Vectorized and parallelizable implementation \n",
        "* A function that compares the outputs of both implementations, ensuring the correctness of the vectorized version.\n",
        "\n",
        "This implementation of MOE was made as an assignment for the Natural Language Processing course at Machine Learning Masters degree at the University of Warsaw. We were given a general structure of how the code needs to look (eg. what classes do we have to implement) but the code was written by myself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ls-lPGw1bHx7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import PretrainedConfig\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "95FnVRnAbNoH"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKebmGHkbUc_",
        "outputId": "1e52d659-ebd8-4490-9df2-e5319867d6c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: [batch_size, seq_len, hidden_size]:  torch.Size([1, 5, 9])\n",
            "Output: [batch_size, seq_len, num_experts]:  torch.Size([1, 5, 7])\n"
          ]
        }
      ],
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, num_experts] - expert routing weights\n",
        "class Router(nn.Module):\n",
        "    \"\"\"\n",
        "    Router module for a Mixture of Experts (MoE) transformer layer.\n",
        "\n",
        "    Given a tensor of token embeddings with shape [batch_size, seq_len, hidden_size],\n",
        "    the router computes a distribution over experts for each token.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of shape [batch_size, seq_len, num_experts] containing\n",
        "        routing weights (after softmax), where each token is assigned to\n",
        "        a subset of experts.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts = config.num_experts\n",
        "\n",
        "        self.expert_embeddings = nn.Parameter(torch.randn(self.num_experts, self.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert_embeddings, nonlinearity='linear')\n",
        "\n",
        "    def chose_top_k_and_replace_rest_with_minus_inf(self, tensor, k, dim):\n",
        "        \"\"\"\n",
        "        Sets all but top-k values along the specified dimension to -inf,\n",
        "        so they are effectively ignored after softmax.\n",
        "        \"\"\"\n",
        "        _, ind = torch.topk(-tensor, k=k, dim = dim)\n",
        "        tensor.scatter_(index=ind, dim=dim, value=float('-inf'))\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = torch.einsum(\"BSH,EH -> BSE\", x, self.expert_embeddings)\n",
        "        result = self.chose_top_k_and_replace_rest_with_minus_inf(result,self.num_experts -  self.num_experts_per_token, 2)\n",
        "        result = F.softmax(result, dim = 2)\n",
        "        return result\n",
        "\n",
        "\n",
        "###TESTING### (if the dimensions in the output match)\n",
        "\n",
        "def test_router(num_experts_per_token, hidden_size, num_experts, seq_len, batch_size):\n",
        "    config = PretrainedConfig(\n",
        "        num_experts_per_token=num_experts_per_token,\n",
        "        hidden_size=hidden_size,\n",
        "        num_experts=num_experts,\n",
        "        batch_size = batch_size,\n",
        "        seq_len = seq_len,\n",
        "    )\n",
        "    router = Router(config)\n",
        "    x = torch.randn(config.batch_size, config.seq_len, config.hidden_size)\n",
        "    print('Input: [batch_size, seq_len, hidden_size]: ', x.shape)\n",
        "    result = router(x)\n",
        "    print('Output: [batch_size, seq_len, num_experts]: ', result.shape)\n",
        "\n",
        "test_router(num_experts_per_token = 3, hidden_size = 9, num_experts = 7, seq_len = 5, batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnT2g7Uebk0R",
        "outputId": "0f419d0d-d9ab-442e-8645-473233e36f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: [batch_size, seq_len, hidden_size]:  torch.Size([1, 7, 9])\n",
            "Output: [batch_size, seq_len, hidden_size]:  torch.Size([1, 7, 9])\n"
          ]
        }
      ],
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class Naive_MoE(nn.Module):\n",
        "    '''\n",
        "    Naive, loop based implementation of Mixture of Experts\n",
        "    Num exerts per token controls how many experts are ascribed to every token\n",
        "    Capacity factor controls how many tokens can go to a single expert\n",
        "    Both input and output are tensors of shape [batch size, sequence length, hidden dim]\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        self.expert1 = nn.Parameter(torch.randn(config.num_experts, config.hidden_size, config.intermediate_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert1, nonlinearity='linear')\n",
        "\n",
        "        self.expert2 = nn.Parameter(torch.randn(config.num_experts,config.intermediate_size, config.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert2, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "        result = torch.zeros((batch_size, seq_len, hidden_size))\n",
        "        weights = self.router(x)\n",
        "\n",
        "        for expert in range(self.num_experts):\n",
        "          token_count = 0\n",
        "          full = False\n",
        "          for batch in range(batch_size):\n",
        "            for token in range(seq_len):\n",
        "              if weights[batch, token, expert] > 0 and full == False:\n",
        "                expert_result = torch.einsum('H, HI -> I', x[batch, token, :], self.expert1[expert])\n",
        "                expert_result = torch.nn.functional.relu(expert_result)\n",
        "                expert_result = torch.einsum('I, IH -> H', expert_result, self.expert2[expert])\n",
        "\n",
        "                result[batch, token, :] += expert_result * weights[batch, token, expert]\n",
        "                token_count += 1\n",
        "                if token_count == expert_capacity:\n",
        "                  full = True\n",
        "        return result\n",
        "\n",
        "###TESTING### (if the dimensions in the output match)\n",
        "\n",
        "def test_Naive_Moe(num_experts_per_token, hidden_size, num_experts, seq_len, batch_size, capacity_factor):\n",
        "    config = PretrainedConfig(\n",
        "        num_experts_per_token=num_experts_per_token,\n",
        "        hidden_size=hidden_size,\n",
        "        num_experts=num_experts,\n",
        "        batch_size = batch_size,\n",
        "        seq_len = seq_len,\n",
        "        capacity_factor = capacity_factor,\n",
        "        intermediate_size=512,\n",
        "    )\n",
        "    moe = Naive_MoE(config)\n",
        "    x = torch.randn(config.batch_size, config.seq_len, config.hidden_size)\n",
        "    print('Input: [batch_size, seq_len, hidden_size]: ', x.shape)\n",
        "    result = moe(x)\n",
        "    print('Output: [batch_size, seq_len, hidden_size]: ', result.shape)\n",
        "\n",
        "test_Naive_Moe(num_experts_per_token = 3, hidden_size = 9, num_experts = 5, seq_len = 7, batch_size = 1, capacity_factor = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1FenjH3bqOm",
        "outputId": "5dff4fd6-d178-462c-e5da-29f2ebed319f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: [batch_size, seq_len, hidden_size]:  torch.Size([2, 5, 7])\n",
            "Output: [batch_size, seq_len, hidden_size]:  torch.Size([2, 5, 7])\n"
          ]
        }
      ],
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class MoE(nn.Module):\n",
        "    '''\n",
        "    Vectorized implementation of Mixture of Experts\n",
        "    Both input and output are tensors of shape [batch size, sequence length, hidden dim]\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        self.expert1 = nn.Parameter(torch.randn(config.num_experts, config.hidden_size, config.intermediate_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert1, nonlinearity='linear')\n",
        "\n",
        "        self.expert2 = nn.Parameter(torch.randn(config.num_experts,config.intermediate_size, config.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert2, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "        self.helper_tensor = torch.tensor(range(config.seq_len * config.batch_size, 0, -1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "\n",
        "        result = torch.zeros((batch_size, seq_len, hidden_size))\n",
        "        weights = self.router(x)\n",
        "        weights = torch.reshape(weights, (batch_size * seq_len, self.num_experts))\n",
        "        tokens = torch.reshape(x, (batch_size * seq_len, hidden_size))\n",
        "        indexes = torch.where(weights > 0, 1, 0)\n",
        "        indexes_segregated = indexes * (self.helper_tensor).unsqueeze(dim = 1)\n",
        "\n",
        "        indexes = chose_top_k_and_replace_rest_with_value(indexes_segregated,self.config.seq_len * self.config.batch_size -  expert_capacity, 0, 0)\n",
        "        #print('po wybraniu top k, \\n', indexes)\n",
        "        indexes = torch.where(indexes > 0, 1, 0)\n",
        "        #print(\"1 tam gdzie dany ekspert bedzie przetwarzal: \\n\",indexes)\n",
        "        weights_result = weights * indexes\n",
        "        #print('Wagi tensorów które ekspert będzie przetważał: \\n', weights_result)\n",
        "        desired_values, desired_indexes = torch.topk(weights_result, expert_capacity, dim = 0)\n",
        "        #print('indexy chcianych przez danego eksperta tensorów: \\n', desired_indexes)\n",
        "        #print('wagi chcianych przez danego eksperta tensorów: \\n', desired_values)\n",
        "        wektor_indeksow_do_index_select = desired_indexes.transpose(0, 1).flatten()\n",
        "        wektor_wag_plaski = desired_values.transpose(0, 1).flatten()\n",
        "        #print('sflatenowane indexy: \\n', wektor_indeksow_do_index_select)\n",
        "        #print('sflatenowane wagi: \\n', wektor_wag_plaski)\n",
        "        experciXcapacityXtokeny = torch.index_select(tokens, 0, wektor_indeksow_do_index_select)\n",
        "        #print('wejsciowe tokeny: \\n', tokeny)\n",
        "        #print('experciXcapacityXtokeny: \\n', experciXcapacityXtokeny)\n",
        "        experciXcapacityXtokeny = torch.reshape(experciXcapacityXtokeny, (self.num_experts, expert_capacity, hidden_size))\n",
        "        #print('experciXcapacityXtokeny trójwymiarowa macierz: \\n', experciXcapacityXtokeny)\n",
        "        #print('experci capacity tokeny shape: ', experciXcapacityXtokeny.shape)\n",
        "        intermidiet_values = torch.einsum(\"ECH, EHI -> ECI\", experciXcapacityXtokeny, self.expert1)\n",
        "        intermidiet_values = torch.nn.functional.relu(intermidiet_values)\n",
        "        intermidiet_values = torch.einsum(\"ECI, EIH -> ECH\", intermidiet_values, self.expert2)\n",
        "        #print('ksztalt po przepuszczeniu przez ekspertow: ', intermidiet_values.shape)\n",
        "        #print('Trójwymiarowa macierz wypluta przez ekspertow: \\n', intermidiet_values)\n",
        "        tokensXhidden = torch.reshape(intermidiet_values, (self.config.num_experts * expert_capacity, hidden_size))\n",
        "        #print('Zreshejpowane po przepuszczeniu przez ekspertow: \\n', tokensXhidden)\n",
        "        #print(tokensXhidden.shape, wektor_wag_plaski.unsqueeze(dim = 1).shape)\n",
        "        tokensXhidden_po_pomnozeniu_przez_wagi = tokensXhidden * wektor_wag_plaski.unsqueeze(dim = 1)\n",
        "        #print('Ksztalt po pomnozeniu przez wektor wag plaski: ', tokensXhidden.shape)\n",
        "        result = torch.zeros((seq_len * batch_size, hidden_size))\n",
        "        result.index_add_(0, wektor_indeksow_do_index_select, tokensXhidden_po_pomnozeniu_przez_wagi)\n",
        "        #print('ostateczny wynik w formie ekspertXcapacity na hidden dim: \\n', result)\n",
        "        #print('result shape ', result.shape)\n",
        "        result = torch.reshape(result, (batch_size, seq_len, hidden_size))\n",
        "\n",
        "\n",
        "\n",
        "        #indexes == macierz (Token x Experci) mówiąca, które tokeny przetwarza który exper\n",
        "        #where = torch.argwhere(indexes)\n",
        "        #print(where)\n",
        "\n",
        "        return result\n",
        "\n",
        "###TESTING###\n",
        "\n",
        "def test_Moe(num_experts_per_token, hidden_size, num_experts, seq_len, batch_size, capacity_factor):\n",
        "    config = PretrainedConfig(\n",
        "        num_experts_per_token=num_experts_per_token,\n",
        "        hidden_size=hidden_size,\n",
        "        num_experts=num_experts,\n",
        "        batch_size = batch_size,\n",
        "        seq_len = seq_len,\n",
        "        capacity_factor = capacity_factor,\n",
        "        intermediate_size=512,\n",
        "    )\n",
        "    moe = MoE(config)\n",
        "    x = torch.randn(config.batch_size, config.seq_len, config.hidden_size)\n",
        "    print('Input: [batch_size, seq_len, hidden_size]: ', x.shape)\n",
        "    result = moe(x)\n",
        "    print('Output: [batch_size, seq_len, hidden_size]: ', result.shape)\n",
        "\n",
        "test_Moe(num_experts_per_token = 2, hidden_size = 7, num_experts = 3, seq_len = 5, batch_size = 2, capacity_factor = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "641RL8bcbuIn",
        "outputId": "0eaf8d9b-4ac3-456d-c243-1951d68fad3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "tensor(8.3819e-09, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "def compare_two_implementations(num_experts_per_token = 2, hidden_size = 7, num_experts = 3, seq_len = 5, batch_size = 2, capacity_factor = 1):\n",
        "    config = PretrainedConfig(\n",
        "      num_experts_per_token=num_experts_per_token,\n",
        "      hidden_size=hidden_size,\n",
        "      num_experts=num_experts,\n",
        "      batch_size = batch_size,\n",
        "      seq_len = seq_len,\n",
        "      capacity_factor = capacity_factor,\n",
        "      intermediate_size=512,\n",
        "  )\n",
        "    naive_moe = Naive_MoE(config)\n",
        "    moe = MoE(config)\n",
        "    router = Router(config)\n",
        "    naive_moe.router = router\n",
        "    moe.router = router\n",
        "    moe.expert1 = naive_moe.expert1\n",
        "    moe.expert2 = naive_moe.expert2\n",
        "    input = torch.rand((batch_size, seq_len, hidden_size))\n",
        "    result_moe = moe(input)\n",
        "    result_naive = naive_moe(input)\n",
        "    print(torch.equal(result_moe, result_naive))\n",
        "    return(torch.max(abs(result_moe - result_naive)))\n",
        "print(compare_two_implementations())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
