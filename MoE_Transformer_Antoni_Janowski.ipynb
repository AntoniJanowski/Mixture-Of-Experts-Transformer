{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ls-lPGw1bHx7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import PretrainedConfig\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "95FnVRnAbNoH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, num_experts] - expert routing weights\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts = config.num_experts\n",
        "\n",
        "        self.expert_embeddings = nn.Parameter(torch.randn(self.num_experts, self.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert_embeddings, nonlinearity='linear')\n",
        "\n",
        "    def chose_top_k_and_replace_rest_with_minus_inf(self, tensor, k, dim):\n",
        "        val, ind = torch.topk(-tensor, k=k, dim = dim)\n",
        "        tensor.scatter_(index=ind, dim=dim, value=float('-inf'))\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = torch.einsum(\"BSH,EH -> BSE\", x, self.expert_embeddings)\n",
        "        result = self.chose_top_k_and_replace_rest_with_minus_inf(result,self.num_experts -  self.num_experts_per_token, 2)\n",
        "        result = F.softmax(result, dim = 2)\n",
        "        return result\n",
        "\n",
        "\n",
        "###TESTING###\n",
        "\n",
        "def test_router(num_experts_per_token, hidden_size, num_experts, seq_len, batch_size):\n",
        "    config = PretrainedConfig(\n",
        "        num_experts_per_token=num_experts_per_token,\n",
        "        hidden_size=hidden_size,\n",
        "        num_experts=num_experts,\n",
        "        batch_size = batch_size,\n",
        "        seq_len = seq_len,\n",
        "    )\n",
        "    router = Router(config)\n",
        "    x = torch.randn(config.batch_size, config.seq_len, config.hidden_size)\n",
        "    print('Input: [batch_size, seq_len, hidden_size]: ', x.shape)\n",
        "    result = router(x)\n",
        "    print('Output: [batch_size, seq_len, num_experts]: ', result.shape)\n",
        "\n",
        "test_router(num_experts_per_token = 3, hidden_size = 9, num_experts = 7, seq_len = 5, batch_size = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKebmGHkbUc_",
        "outputId": "1e52d659-ebd8-4490-9df2-e5319867d6c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [batch_size, seq_len, hidden_size]:  torch.Size([1, 5, 9])\n",
            "Output: [batch_size, seq_len, num_experts]:  torch.Size([1, 5, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chose_top_k_and_replace_rest_with_value(tensor, k, dim, value):\n",
        "    val, ind = torch.topk(-tensor, k=k, dim = dim)\n",
        "    tensor.scatter_(index=ind, dim=dim, value=value)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "ysyej40rbZSg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class Naive_MoE(nn.Module): #wersja gdzie eksperci to dwie macierze\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        self.expert1 = nn.Parameter(torch.randn(config.num_experts, config.hidden_size, config.intermediate_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert1, nonlinearity='linear')\n",
        "\n",
        "        self.expert2 = nn.Parameter(torch.randn(config.num_experts,config.intermediate_size, config.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert2, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "        result = torch.zeros((batch_size, seq_len, hidden_size))\n",
        "        weights = self.router(x)\n",
        "\n",
        "        for expert in range(self.num_experts):\n",
        "          token_count = 0\n",
        "          full = False\n",
        "          for batch in range(batch_size):\n",
        "            for token in range(seq_len):\n",
        "              if weights[batch, token, expert] > 0 and full == False:\n",
        "                expert_result = torch.einsum('H, HI -> I', x[batch, token, :], self.expert1[expert])\n",
        "                expert_result = torch.nn.functional.relu(expert_result)\n",
        "                expert_result = torch.einsum('I, IH -> H', expert_result, self.expert2[expert])\n",
        "\n",
        "                result[batch, token, :] += expert_result * weights[batch, token, expert]\n",
        "                token_count += 1\n",
        "                if token_count == expert_capacity:\n",
        "                  full = True\n",
        "        return result\n",
        "\n",
        "###TESTING###\n",
        "\n",
        "def test_Naive_Moe(num_experts_per_token, hidden_size, num_experts, seq_len, batch_size, capacity_factor):\n",
        "    config = PretrainedConfig(\n",
        "        num_experts_per_token=num_experts_per_token,\n",
        "        hidden_size=hidden_size,\n",
        "        num_experts=num_experts,\n",
        "        batch_size = batch_size,\n",
        "        seq_len = seq_len,\n",
        "        capacity_factor = capacity_factor,\n",
        "        intermediate_size=512,\n",
        "    )\n",
        "    moe = Naive_MoE(config)\n",
        "    x = torch.randn(config.batch_size, config.seq_len, config.hidden_size)\n",
        "    print('Input: [batch_size, seq_len, hidden_size]: ', x.shape)\n",
        "    result = moe(x)\n",
        "    print('Output: [batch_size, seq_len, hidden_size]: ', result.shape)\n",
        "\n",
        "test_Naive_Moe(num_experts_per_token = 3, hidden_size = 9, num_experts = 5, seq_len = 7, batch_size = 1, capacity_factor = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnT2g7Uebk0R",
        "outputId": "0f419d0d-d9ab-442e-8645-473233e36f75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [batch_size, seq_len, hidden_size]:  torch.Size([1, 7, 9])\n",
            "Output: [batch_size, seq_len, hidden_size]:  torch.Size([1, 7, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class MoE(nn.Module): #dziala!\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        # You can change experts representation if you want\n",
        "        #self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])\n",
        "        self.expert1 = nn.Parameter(torch.randn(config.num_experts, config.hidden_size, config.intermediate_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert1, nonlinearity='linear')\n",
        "\n",
        "        self.expert2 = nn.Parameter(torch.randn(config.num_experts,config.intermediate_size, config.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert2, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "        self.helper_tensor = torch.tensor(range(config.seq_len * config.batch_size, 0, -1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "        #print('capacity = ', expert_capacity)\n",
        "        result = torch.zeros((batch_size, seq_len, hidden_size))\n",
        "        weights = self.router(x)\n",
        "        weights = torch.reshape(weights, (batch_size * seq_len, self.num_experts))\n",
        "        tokeny = torch.reshape(x, (batch_size * seq_len, hidden_size))\n",
        "        #print('wagi \\n', weights)\n",
        "        indexes = torch.where(weights > 0, 1, 0)\n",
        "        #print('podmieniam wagi na 1 \\n',indexes)\n",
        "        #print(indexes.shape, (self.helper_tensor.unsqueeze(dim = 1).shape))\n",
        "        indexes_segregated = indexes * (self.helper_tensor).unsqueeze(dim = 1)\n",
        "        #print('mnoze i dostaje numerki wierszy \\n',indexes_segregated)\n",
        "        indexes = chose_top_k_and_replace_rest_with_value(indexes_segregated,self.config.seq_len * self.config.batch_size -  expert_capacity, 0, 0)\n",
        "        #print('po wybraniu top k, \\n', indexes)\n",
        "        indexes = torch.where(indexes > 0, 1, 0)\n",
        "        #print(\"1 tam gdzie dany ekspert bedzie przetwarzal: \\n\",indexes)\n",
        "        weights_result = weights * indexes\n",
        "        #print('Wagi tensorów które ekspert będzie przetważał: \\n', weights_result)\n",
        "        desired_values, desired_indexes = torch.topk(weights_result, expert_capacity, dim = 0)\n",
        "        #print('indexy chcianych przez danego eksperta tensorów: \\n', desired_indexes)\n",
        "        #print('wagi chcianych przez danego eksperta tensorów: \\n', desired_values)\n",
        "        wektor_indeksow_do_index_select = desired_indexes.transpose(0, 1).flatten()\n",
        "        wektor_wag_plaski = desired_values.transpose(0, 1).flatten()\n",
        "        #print('sflatenowane indexy: \\n', wektor_indeksow_do_index_select)\n",
        "        #print('sflatenowane wagi: \\n', wektor_wag_plaski)\n",
        "        experciXcapacityXtokeny = torch.index_select(tokeny, 0, wektor_indeksow_do_index_select)\n",
        "        #print('wejsciowe tokeny: \\n', tokeny)\n",
        "        #print('experciXcapacityXtokeny: \\n', experciXcapacityXtokeny)\n",
        "        experciXcapacityXtokeny = torch.reshape(experciXcapacityXtokeny, (self.num_experts, expert_capacity, hidden_size))\n",
        "        #print('experciXcapacityXtokeny trójwymiarowa macierz: \\n', experciXcapacityXtokeny)\n",
        "        #print('experci capacity tokeny shape: ', experciXcapacityXtokeny.shape)\n",
        "        intermidiet_values = torch.einsum(\"ECH, EHI -> ECI\", experciXcapacityXtokeny, self.expert1)\n",
        "        intermidiet_values = torch.nn.functional.relu(intermidiet_values)\n",
        "        intermidiet_values = torch.einsum(\"ECI, EIH -> ECH\", intermidiet_values, self.expert2)\n",
        "        #print('ksztalt po przepuszczeniu przez ekspertow: ', intermidiet_values.shape)\n",
        "        #print('Trójwymiarowa macierz wypluta przez ekspertow: \\n', intermidiet_values)\n",
        "        tokensXhidden = torch.reshape(intermidiet_values, (self.config.num_experts * expert_capacity, hidden_size))\n",
        "        #print('Zreshejpowane po przepuszczeniu przez ekspertow: \\n', tokensXhidden)\n",
        "        #print(tokensXhidden.shape, wektor_wag_plaski.unsqueeze(dim = 1).shape)\n",
        "        tokensXhidden_po_pomnozeniu_przez_wagi = tokensXhidden * wektor_wag_plaski.unsqueeze(dim = 1)\n",
        "        #print('Ksztalt po pomnozeniu przez wektor wag plaski: ', tokensXhidden.shape)\n",
        "        result = torch.zeros((seq_len * batch_size, hidden_size))\n",
        "        result.index_add_(0, wektor_indeksow_do_index_select, tokensXhidden_po_pomnozeniu_przez_wagi)\n",
        "        #print('ostateczny wynik w formie ekspertXcapacity na hidden dim: \\n', result)\n",
        "        #print('result shape ', result.shape)\n",
        "        result = torch.reshape(result, (batch_size, seq_len, hidden_size))\n",
        "\n",
        "\n",
        "\n",
        "        #indexes == macierz (Token x Experci) mówiąca, które tokeny przetwarza który exper\n",
        "        #where = torch.argwhere(indexes)\n",
        "        #print(where)\n",
        "\n",
        "        return result\n",
        "\n",
        "###TESTING###\n",
        "\n",
        "def test_Moe(num_experts_per_token, hidden_size, num_experts, seq_len, batch_size, capacity_factor):\n",
        "    config = PretrainedConfig(\n",
        "        num_experts_per_token=num_experts_per_token,\n",
        "        hidden_size=hidden_size,\n",
        "        num_experts=num_experts,\n",
        "        batch_size = batch_size,\n",
        "        seq_len = seq_len,\n",
        "        capacity_factor = capacity_factor,\n",
        "        intermediate_size=512,\n",
        "    )\n",
        "    moe = MoE(config)\n",
        "    x = torch.randn(config.batch_size, config.seq_len, config.hidden_size)\n",
        "    print('Input: [batch_size, seq_len, hidden_size]: ', x.shape)\n",
        "    result = moe(x)\n",
        "    print('Output: [batch_size, seq_len, hidden_size]: ', result.shape)\n",
        "\n",
        "test_Moe(num_experts_per_token = 2, hidden_size = 7, num_experts = 3, seq_len = 5, batch_size = 2, capacity_factor = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1FenjH3bqOm",
        "outputId": "5dff4fd6-d178-462c-e5da-29f2ebed319f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [batch_size, seq_len, hidden_size]:  torch.Size([2, 5, 7])\n",
            "Output: [batch_size, seq_len, hidden_size]:  torch.Size([2, 5, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_two_implementations(num_experts_per_token = 2, hidden_size = 7, num_experts = 3, seq_len = 5, batch_size = 2, capacity_factor = 1):\n",
        "    config = PretrainedConfig(\n",
        "      num_experts_per_token=num_experts_per_token,\n",
        "      hidden_size=hidden_size,\n",
        "      num_experts=num_experts,\n",
        "      batch_size = batch_size,\n",
        "      seq_len = seq_len,\n",
        "      capacity_factor = capacity_factor,\n",
        "      intermediate_size=512,\n",
        "  )\n",
        "    naive_moe = Naive_MoE(config)\n",
        "    moe = MoE(config)\n",
        "    router = Router(config)\n",
        "    naive_moe.router = router\n",
        "    moe.router = router\n",
        "    moe.expert1 = naive_moe.expert1\n",
        "    moe.expert2 = naive_moe.expert2\n",
        "    input = torch.rand((batch_size, seq_len, hidden_size))\n",
        "    result_moe = moe(input)\n",
        "    result_naive = naive_moe(input)\n",
        "    print(torch.equal(result_moe, result_naive))\n",
        "    return(torch.max(abs(result_moe - result_naive)))\n",
        "print(compare_two_implementations())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "641RL8bcbuIn",
        "outputId": "0eaf8d9b-4ac3-456d-c243-1951d68fad3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "tensor(8.3819e-09, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    }
  ]
}